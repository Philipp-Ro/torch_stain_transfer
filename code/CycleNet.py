import torch
from torch.autograd import Variable
import torch.optim as optim
import numpy as np
import loader
from torch.utils.data import DataLoader

class model(torch.nn.Module):
    def __init__(self,params, generator_G, generator_F, discriminator_X, discriminator_Y, disc_X_optimizer, disc_Y_optimizer, gen_optimizer):
        super(model, self).__init__()
        # initialse the generator G and F
        # gen_F transfers from doman Y -> X
        # gen_G transfers from doman X -> Y
        # 
        # initialize the discriminator X and Y
        # disc_X distinguishes between real and fake in the X domain 
        # disc_Y distinguishes between real and fake in the Y domain 
        #
        #
        self.gen_G = generator_G
        self.gen_F = generator_F
        self.disc_X = discriminator_X
        self.disc_Y = discriminator_Y
        self.disc_X_optimizer = disc_X_optimizer
        self.disc_Y_optimizer = disc_Y_optimizer
        self.gen_optimizer = gen_optimizer
        self.params = params


    def fit(self):
        Tensor = torch.cuda.FloatTensor
        criterion_GAN = torch.nn.MSELoss().cuda()
        criterion_cycle = torch.nn.L1Loss().cuda()
        criterion_identity = torch.nn.L1Loss().cuda()

        
        for epoch in range(self.params['num_epochs']):

            # the dataset is set up he coppes images out of the original image i the set size 
            # each epoch he takes a new slice of the original image 
            # recomended sizes [64,64] / [128,128] / [256, 256]  
            train_path = self.params['train_dir']
            HE_img_dir = "{}{}".format(train_path,'/HE_imgs/HE')
            IHC_img_dir = "{}{}".format(train_path,'/IHC_imgs/IHC')

            train_data = loader.stain_transfer_dataset( epoch = epoch,
                                                        num_epochs = self.params['num_epochs'],
                                                        HE_img_dir = HE_img_dir,
                                                        IHC_img_dir = IHC_img_dir,
                                                        img_size= self.params['img_size'],
                                           )

            # get dataloader
            train_data_loader = DataLoader(train_data, batch_size=1, shuffle=False) 

            if(epoch + 1) > self.params['decay_epoch']:
                self.disc_X_optimizer.param_groups[0]['lr'] -= self.params['learn_rate_disc'] / (self.params['num_epochs'] - self.params['decay_epoch'])
                self.disc_Y_optimizer.param_groups[0]['lr'] -= self.params['learn_rate_disc'] / (self.params['num_epochs'] - self.params['decay_epoch'])
                self.gen_optimizer.param_groups[0]['lr'] -= self.params['learn_rate_gen'] / (self.params['num_epochs'] - self.params['decay_epoch'])
            
            for i, (real_HE, real_IHC) in enumerate(train_data_loader):

                # Adversarial ground truths
                valid = Tensor(np.ones((real_HE.size(0)))) # requires_grad = False. Default.
                fake = Tensor(np.zeros((real_HE.size(0)))) # requires_grad = False. Default.
                
                # -----------------------------------------------------------------------------------------
                # Train Generators
                # ------------------------------------------------------------------------------------------
                self.gen_G.train() # train mode
                self.gen_G.train() # train mode
                
                self.gen_optimizer.zero_grad() # Integrated optimizer(G_AB, G_BA)
                

                fake_IHC = self.gen_G(real_HE) # fake_B is fake-photo that generated by real monet-drawing
                

                # Identity Loss
                loss_id_HE = criterion_identity(self.gen_F(real_IHC), real_IHC) # If you put A into a generator that creates A with B,
                loss_id_IHC = criterion_identity(self.gen_G(real_HE), real_HE) # then of course A must come out as it is.
                                                                    # Taking this into consideration, add an identity loss that simply compares 'A and A' (or 'B and B').
                loss_identity = (loss_id_HE + loss_id_IHC)/2
                
                # GAN Loss
                fake_IHC = self.gen_G(real_HE) # fake_B is fake-photo that generated by real monet-drawing
               
                loss_gen_G = criterion_GAN(self.disc_Y(fake_IHC), valid) # tricking the 'fake-B' into 'real-B'

                fake_HE = self.gen_F(real_IHC)
                
                loss_gen_F = criterion_GAN(self.disc_X(fake_HE), valid) # tricking the 'fake-A' into 'real-A'
                
                loss_GAN = (loss_gen_G +  loss_gen_F)/2
                
                # Cycle Loss
                cycled_IHC = self.gen_G(fake_HE) # recov_A is fake-monet-drawing that generated by fake-photo
                loss_cycle_G = criterion_cycle(cycled_IHC, real_IHC) # Reduces the difference between the restored image and the real image
                cycled_HE = self.gen_F(fake_IHC)
                loss_cycle_F = criterion_cycle(cycled_HE, real_IHC)
                
                loss_cycle = (loss_cycle_G + loss_cycle_F)/2
                
                # ------> Total Loss
                loss_G = loss_GAN + (self.params['cycle_lambda']*loss_cycle) + (self.params['identity_lambda']*loss_identity) # multiply suggested weight(default cycle loss weight : 10, default identity loss weight : 5)
                
                loss_G.backward()
                self.gen_optimizer.step()
                
                # ---------------------------------------------------------------------------------
                # Train Discriminator A
                # ---------------------------------------------------------------------------------
                self.disc_X_optimizer.zero_grad()
            
                loss_real = criterion_GAN(self.disc_X(real_HE), valid) # train to discriminate real images as real
                loss_fake = criterion_GAN(self.disc_X(fake_HE.detach()), fake) # train to discriminate fake images as fake
                
                loss_disc_X = (loss_real + loss_fake)/2
                
                loss_disc_X .backward()
                self.disc_X_optimizer.step()

                # ---------------------------------------------------------------------------------------
                # Train Discriminator B
                # ----------------------------------------------------------------------------------------
                self.disc_Y_optimizer.zero_grad()
            
                loss_real = criterion_GAN(self.disc_Y(real_IHC), valid) # train to discriminate real images as real
                loss_fake = criterion_GAN(self.disc_Y(fake_IHC.detach()), fake) # train to discriminate fake images as fake
                
                loss_disc_Y = (loss_real + loss_fake)/2
                
                loss_disc_Y .backward()
                self.disc_Y_optimizer.step()
                
                # ------> Total Loss
                loss_D = (loss_disc_X + loss_disc_Y )/2
            
                # -----------------------------------------------------------------------------------------
                # Show Progress
                # --------------------------------------------------------------------------------------------
                if (i+1) % 50 == 0:
                    
                    print('[Epoch %d/%d] [Batch %d/%d] [D loss : %f] [G loss : %f - (adv : %f, cycle : %f, identity : %f)]'
                            %(epoch+1,self.params['num_epochs'],       # [Epoch -]
                            i+1,len(train_data_loader),   # [Batch -]
                            loss_D.item(),       # [D loss -]
                            loss_G.item(),       # [G loss -]
                            loss_GAN.item(),     # [adv -]
                            loss_cycle.item(),   # [cycle -]
                            loss_identity.item(),# [identity -]
                            ))

        return self.gen_G, self.gen_F, self.disc_X, self.disc_Y



def weights_init_normal(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        torch.nn.init.normal_(m.weight.data, 0.0, 0.02) # reset Conv2d's weight(tensor) with Gaussian Distribution
        if hasattr(m, 'bias') and m.bias is not None:
            torch.nn.init.constant_(m.bias.data, 0.0) # reset Conv2d's bias(tensor) with Constant(0)
        elif classname.find('BatchNorm2d') != -1:
            torch.nn.init.normal_(m.weight.data, 1.0, 0.02) # reset BatchNorm2d's weight(tensor) with Gaussian Distribution
            torch.nn.init.constant_(m.bias.data, 0.0) # reset BatchNorm2d's bias(tensor) with Constant(0)

