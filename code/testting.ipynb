{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import loader\n",
    "path_HE = 'C:/Users/phili/OneDrive/Uni/WS_22/Masterarbeit/Masterarbeit_Code_Philipp_Rosin/Data_set_BCI_challange/train/HE_imgs/HE'\n",
    "path_IHC = 'C:/Users/phili/OneDrive/Uni/WS_22/Masterarbeit/Masterarbeit_Code_Philipp_Rosin/Data_set_BCI_challange/train/IHC_imgs/IHC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_data = loader.stain_transfer_dataset( epoch = 1,\n",
    "                                                        num_epochs = 1,\n",
    "                                                        HE_img_dir = path_HE,\n",
    "                                                        IHC_img_dir = path_IHC,\n",
    "                                                        img_size=  [64,64],\n",
    "                                           )\n",
    "train_data_loader = DataLoader(train_data, batch_size=1, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 64])\n",
      "torch.Size([1, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "HE_img, IHC_img = next(iter(train_data_loader))\n",
    "print (HE_img.shape)\n",
    "print (IHC_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "####### patch embedding ###########\n",
    "#   - in_channels are the input channels of the images for rgb = 3\n",
    "#   - patch_size is the sizes of the patches for the embedding \n",
    "#   - img_size is the size of the input image (has to be quadratic)\n",
    "#   - embedding_dim are the dimentions for the embedding with embedding_dim=0 it will be automaticly calculated \n",
    "    def __init__(self, in_channels, patch_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "# 1) the conv layer with kernel_size = stride writes out patches of the kernels the embedding_dim should be choosen as (patch_size**2)* in_channel so that no information is lost          \n",
    "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=self.embedding_dim,\n",
    "                                 kernel_size=patch_size,\n",
    "                                 stride=patch_size,\n",
    "                                 padding=0)\n",
    "\n",
    "# 2) flatten the feature map into 1D\n",
    "        self.flatten = nn.Flatten(start_dim=2,\n",
    "                                  end_dim=3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        \n",
    "        x_patched = self.patcher(x)\n",
    "        x_flattened = self.flatten(x_patched) \n",
    "# 3) permute the output tensor so that it has the form [batch_size, embedding_dim, num_patches]      \n",
    "        return x_flattened\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def getPositionEmbedding(embedding_dim, num_patches, n=10000):\n",
    "    # the n variable is scalling the values in the positional embedding in the attention is all you need paper n=10000 was choosen \n",
    "    p_embedding = torch.zeros((embedding_dim, num_patches))\n",
    "    p_embedding = p_embedding.cuda()\n",
    "    for k in range(embedding_dim):\n",
    "        for i in torch.arange(int(num_patches/2)):\n",
    "            denominator = np.power(n, 2*i/num_patches)\n",
    "            p_embedding[k, 2*i] = np.sin(k/denominator)\n",
    "            p_embedding[k, 2*i+1] = np.cos(k/denominator)\n",
    "    \n",
    "    return torch.unsqueeze(p_embedding, dim=0)\n",
    "\n",
    "x = getPositionEncoding(embedding_dim=764, num_patches=16, n=10)\n",
    "\n",
    "\n",
    "print(x.get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 764, 16])\n"
     ]
    }
   ],
   "source": [
    "patches = PatchEmbedding(in_channels=3,patch_size=16, embedding_dim=764)\n",
    "patches.cuda()\n",
    "y = patches(HE_img)\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_model = embedding_dim\n",
    "# nhead = attention heads \n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer\n",
    "\n",
    "\n",
    "transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768,\n",
    "                                                       nhead=12,\n",
    "                                                       dim_feedforward=2048,\n",
    "                                                       dropout=0.1,\n",
    "                                                       activation=\"gelu\",\n",
    "                                                       batch_first=True,\n",
    "                                                       norm_first=True)\n",
    "\n",
    "transformer_decoder_layer = nn.TransformerDecoderLayer(d_model=768,\n",
    "                                                       nhead=12,\n",
    "                                                       dim_feedforward=2048,\n",
    "                                                       dropout=0.1,\n",
    "                                                       activation=\"gelu\",\n",
    "                                                       batch_first=True,\n",
    "                                                       norm_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_encoder = nn.TransformerEncoder(\n",
    "    encoder_layer=transformer_encoder_layer,\n",
    "    num_layers=12)\n",
    "\n",
    "transformer_decoder = nn.TransformerEncoder(\n",
    "    encoder_layer=transformer_decoder_layer,\n",
    "    num_layers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "###### GENERATOR CLASS #############\n",
    "# 1) set up the embedding of input img :\n",
    "#        - img_size is the size of the input inmage of the generator \n",
    "#        - embedding_dim : the dimentions used also in the transformer\n",
    "#        - patch_size : sizes of patches cut by the embedding [img_size % patch_size != 0]\n",
    "#        - create positional embedding\n",
    "#\n",
    "# 2) set up the the transformer encoder layer class :\n",
    "#        ---> https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer\n",
    "#        - the d_model = embedding_dim so that the dimentions of the embedded image and the transformer network match\n",
    "#        - nhead sets the number of heads for self attention in a transformer block \n",
    "#\n",
    "# 3) set up encoder class:\n",
    "#        --->  https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder\n",
    "#        - use the encoder-layer set up in 2) \n",
    "#        - num_layers defines the number of encoder-layers in the encoder\n",
    "\n",
    "    def __init__(self,img_size,embedding_dim, patch_size, in_channels, dropout_embedding, nhead,num_layers):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.dropout_embedding = dropout_embedding\n",
    "        self.nhead = nhead\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "\n",
    "        #### testting the compatebility for img_size and patch_site \n",
    "        if img_size % patch_size == 0:\n",
    "            self.patch_size = patch_size\n",
    "        else : \n",
    "            print('img_size / patch_size has to have no rest')\n",
    "\n",
    "        # number of patches in image for given patchsize \n",
    "        num_patches = (img_size * img_size) // patch_size**2 \n",
    "        # number of valiables in input image ( num_channels* img_height* img_width)\n",
    "        num_values = in_channels * img_size**2\n",
    "\n",
    "        if num_values % num_patches == 0:\n",
    "            self.embedding_dim =  int(num_values/num_patches)\n",
    "        else:\n",
    "            print('num_values / patch_num has to have no rest')\n",
    "\n",
    "        # create patches from the imput image the output by the PatchEmbedding is : [batch_size, num_patches, embedding_dim ]\n",
    "        # where as the embedding_dim is choosen as patch_size**2 * in_channels \n",
    "        self.patch_embedding = PatchEmbedding(in_channels=self.in_channels,patch_size=self.patch_size, embedding_dim=self.embedding_dim)\n",
    "    \n",
    "        self.num_patches = (img_size * img_size) // patch_size**2 \n",
    "        # positional embedding is added to keep the informatiuon of the order of the patches \n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, self.num_patches, self.embedding_dim))\n",
    "\n",
    "        self.embedding_dropout = nn.Dropout(p=self.dropout_embedding)\n",
    "\n",
    "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model= self.embedding_dim,\n",
    "                                                               nhead=self.nhead ,\n",
    "                                                               dim_feedforward=2048,\n",
    "                                                               dropout=0.1,\n",
    "                                                               activation=\"gelu\",\n",
    "                                                               batch_first=True,\n",
    "                                                               norm_first=True)\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "                                                    encoder_layer=self.transformer_encoder_layer,\n",
    "                                                    num_layers=self.num_layers)\n",
    "        \n",
    "        self.upsample = nn.PixelShuffle(self.num_patches)\n",
    "        \n",
    "        #self.linear = nn.Sequential(nn.Conv2d(self.embedding_dim, 3, 1, 1, 0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # Create the patch embedding\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        x = self.positional_embedding + x\n",
    "\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        x = x.view(1, self.embedding_dim, int(math.sqrt(self.num_patches)), int(math.sqrt(self.num_patches)))\n",
    "\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary \n",
    "\n",
    "vit = Generator(img_size= 64,\n",
    "                embedding_dim=0,\n",
    "                patch_size=16,\n",
    "                in_channels=3,\n",
    "                dropout_embedding=0.1,\n",
    "                nhead= 4,\n",
    "                num_layers=6\n",
    "                \n",
    "                )\n",
    "vit.cuda()\n",
    "fake_IHC = vit(HE_img)\n",
    "print(fake_IHC.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoderLayer(\n",
       "  (self_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "  (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768,\n",
    "                                                       nhead=12,\n",
    "                                                       dim_feedforward=2048,\n",
    "                                                       dropout=0.1,\n",
    "                                                       activation=\"gelu\",\n",
    "                                                       batch_first=True,\n",
    "                                                       norm_first=True)\n",
    "transformer_encoder_layer.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "upsample = nn.PixelShuffle(16)\n",
    "x = fake_IHC.permute(0,2,1).view(1,768,4,4)\n",
    "y = upsample(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m768\u001b[39m)\n\u001b[0;32m      3\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\u001b[39m3\u001b[39;49m,\u001b[39m64\u001b[39;49m,\u001b[39m64\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      6\u001b[0m conv_layer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mConv2d(in_channels \u001b[39m=\u001b[39m\u001b[39m768\u001b[39m,\n\u001b[0;32m      7\u001b[0m                        out_channels\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m,\n\u001b[0;32m      8\u001b[0m                        kernel_size\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m      9\u001b[0m                        stride \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m     10\u001b[0m                        padding\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "x = torch.zeros(1, 16, 768)\n",
    "\n",
    "x = x.permute(0, 2, 1)\n",
    "x = x.view(-1,3,64,64)\n",
    "print(x.shape)\n",
    "conv_layer = nn.Conv2d(in_channels =768,\n",
    "                       out_channels= 3,\n",
    "                       kernel_size= 1,\n",
    "                       stride = 1,\n",
    "                       padding= 0)\n",
    "\n",
    "out  = conv_layer (x)\n",
    "print (out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {}\n",
    "a['num'] = []\n",
    "for n in range (10):\n",
    "    a['num'].append(n)\n",
    "\n",
    "\n",
    "# open file for writing\n",
    "f = open(\"dict.txt\",\"w\")\n",
    "\n",
    "# write file\n",
    "f.write( str(a) )\n",
    "\n",
    "# close file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 8.00 GiB total capacity; 7.19 GiB already allocated; 0 bytes free; 7.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m params \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mget_config_from_yaml(\u001b[39m'\u001b[39m\u001b[39mC:/Users/phili/OneDrive/Uni/WS_22/Masterarbeit/Masterarbeit_Code_Philipp_Rosin/torch_stain_transfer/code/config.yaml\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m model \u001b[39m=\u001b[39m conv_models\u001b[39m.\u001b[39mGeneratorResNet(\u001b[39m3\u001b[39m, num_residual_blocks\u001b[39m=\u001b[39m\u001b[39m9\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m model \u001b[39m=\u001b[39m  model\u001b[39m.\u001b[39;49mcuda()\n\u001b[0;32m     14\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(gen_G_path))\n\u001b[0;32m     15\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\phili\\OneDrive\\Uni\\WS_22\\Masterarbeit\\Masterarbeit_Code_Philipp_Rosin\\torch_stain_transfer\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:749\u001b[0m, in \u001b[0;36mModule.cuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m    733\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    734\u001b[0m \n\u001b[0;32m    735\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 749\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[1;32mc:\\Users\\phili\\OneDrive\\Uni\\WS_22\\Masterarbeit\\Masterarbeit_Code_Philipp_Rosin\\torch_stain_transfer\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\phili\\OneDrive\\Uni\\WS_22\\Masterarbeit\\Masterarbeit_Code_Philipp_Rosin\\torch_stain_transfer\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\phili\\OneDrive\\Uni\\WS_22\\Masterarbeit\\Masterarbeit_Code_Philipp_Rosin\\torch_stain_transfer\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    661\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 664\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    665\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    666\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\phili\\OneDrive\\Uni\\WS_22\\Masterarbeit\\Masterarbeit_Code_Philipp_Rosin\\torch_stain_transfer\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:749\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m    733\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    734\u001b[0m \n\u001b[0;32m    735\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 749\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 8.00 GiB total capacity; 7.19 GiB already allocated; 0 bytes free; 7.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch \n",
    "import loader \n",
    "import conv_models\n",
    "from torch.utils.data import DataLoader\n",
    "import utils\n",
    "from torchmetrics import StructuralSimilarityIndexMeasure\n",
    "from torchmetrics import PeakSignalNoiseRatio\n",
    "torch.cuda.empty_cache()\n",
    "gen_G_path = 'C:/Users/phili/OneDrive/Uni/WS_22/Masterarbeit/Masterarbeit_Code_Philipp_Rosin/torch_stain_transfer/experiment-results/Test_01/Generator_G_weights.pth'\n",
    "params = utils.get_config_from_yaml('C:/Users/phili/OneDrive/Uni/WS_22/Masterarbeit/Masterarbeit_Code_Philipp_Rosin/torch_stain_transfer/code/config.yaml')\n",
    "model = conv_models.GeneratorResNet(3, num_residual_blocks=9)\n",
    "model =  model.cuda()\n",
    "\n",
    "model.load_state_dict(torch.load(gen_G_path))\n",
    "model.eval()\n",
    "\n",
    "# set up result vector \n",
    "result = {}\n",
    "result['epoch'] = []\n",
    "result['ssim_mean'] = []\n",
    "result['ssim_std'] = []\n",
    "result['psnr_mean'] = []\n",
    "result['psnr_std'] = []\n",
    "\n",
    "# set up test data dirs \n",
    "test_path = params['test_dir']\n",
    "HE_img_dir = \"{}{}\".format(test_path,'/HE_imgs/HE')\n",
    "IHC_img_dir = \"{}{}\".format(test_path,'/IHC_imgs/IHC')\n",
    "\n",
    "for epoch in range(params['num_epochs']):\n",
    "    \n",
    "    result['epoch'].append(epoch)\n",
    "    test_data = loader.stain_transfer_dataset(  epoch = epoch,\n",
    "                                                num_epochs = params['num_epochs'],\n",
    "                                                HE_img_dir = HE_img_dir,\n",
    "                                                IHC_img_dir = IHC_img_dir,\n",
    "                                                img_size= params['img_size'],\n",
    "                                                )\n",
    "    \n",
    "    test_data_loader = DataLoader(test_data, batch_size=1, shuffle=False) \n",
    "\n",
    "    ssim = StructuralSimilarityIndexMeasure(data_range=1.0)\n",
    "    ssim = ssim.cuda()\n",
    "    ssim_scores = []\n",
    "\n",
    "    psnr = PeakSignalNoiseRatio()\n",
    "    psnr = psnr.cuda()\n",
    "    psnr_scores = []\n",
    "\n",
    "    for i, (real_HE, real_IHC) in enumerate(test_data_loader):\n",
    "        \n",
    "        fake_IHC = model(real_HE)\n",
    "        \n",
    "        ssim_scores.append(ssim(fake_IHC, real_IHC))\n",
    "        psnr_scores.append(psnr(fake_IHC, real_IHC))\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    result['ssim_mean'].append(np.mean(ssim_scores))\n",
    "    result['ssim_std'].append(np.std(ssim_scores))\n",
    "\n",
    "    result['psnr_mean'].append(np.mean(psnr_scores))\n",
    "    result['psnr_std'].append(np.std(psnr_scores))\n",
    "\n",
    "# open file for writing\n",
    "f = open(output_folder_path,\"w\")\n",
    "\n",
    "# write file\n",
    "f.write( str(result) )\n",
    "\n",
    "# close file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 8.00 GiB total capacity; 7.19 GiB already allocated; 0 bytes free; 7.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m params \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mget_config_from_yaml(\u001b[39m'\u001b[39m\u001b[39mC:/Users/phili/OneDrive/Uni/WS_22/Masterarbeit/Masterarbeit_Code_Philipp_Rosin/torch_stain_transfer/code/config.yaml\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m model \u001b[39m=\u001b[39m conv_models\u001b[39m.\u001b[39mGeneratorResNet(\u001b[39m3\u001b[39m, num_residual_blocks\u001b[39m=\u001b[39m\u001b[39m9\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m model \u001b[39m=\u001b[39m  model\u001b[39m.\u001b[39;49mcuda()\n\u001b[0;32m     17\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(gen_G_path))\n\u001b[0;32m     18\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\phili\\OneDrive\\Uni\\WS_22\\Masterarbeit\\Masterarbeit_Code_Philipp_Rosin\\torch_stain_transfer\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:749\u001b[0m, in \u001b[0;36mModule.cuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m    733\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    734\u001b[0m \n\u001b[0;32m    735\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 749\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[1;32mc:\\Users\\phili\\OneDrive\\Uni\\WS_22\\Masterarbeit\\Masterarbeit_Code_Philipp_Rosin\\torch_stain_transfer\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\phili\\OneDrive\\Uni\\WS_22\\Masterarbeit\\Masterarbeit_Code_Philipp_Rosin\\torch_stain_transfer\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\phili\\OneDrive\\Uni\\WS_22\\Masterarbeit\\Masterarbeit_Code_Philipp_Rosin\\torch_stain_transfer\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    661\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 664\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    665\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    666\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\phili\\OneDrive\\Uni\\WS_22\\Masterarbeit\\Masterarbeit_Code_Philipp_Rosin\\torch_stain_transfer\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:749\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m    733\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    734\u001b[0m \n\u001b[0;32m    735\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 749\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 8.00 GiB total capacity; 7.19 GiB already allocated; 0 bytes free; 7.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "import gc\n",
    "import torch \n",
    "import loader \n",
    "import conv_models\n",
    "from torch.utils.data import DataLoader\n",
    "import utils\n",
    "from torchmetrics import StructuralSimilarityIndexMeasure\n",
    "from torchmetrics import PeakSignalNoiseRatio\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "gen_G_path = 'C:/Users/phili/OneDrive/Uni/WS_22/Masterarbeit/Masterarbeit_Code_Philipp_Rosin/torch_stain_transfer/experiment-results/Test_01/Generator_G_weights.pth'\n",
    "params = utils.get_config_from_yaml('C:/Users/phili/OneDrive/Uni/WS_22/Masterarbeit/Masterarbeit_Code_Philipp_Rosin/torch_stain_transfer/code/config.yaml')\n",
    "model = conv_models.GeneratorResNet(3, num_residual_blocks=9)\n",
    "model =  model.cuda()\n",
    "\n",
    "model.load_state_dict(torch.load(gen_G_path))\n",
    "model.eval()\n",
    "\n",
    "# set up result vector \n",
    "result = {}\n",
    "result['epoch'] = []\n",
    "result['ssim_mean'] = []\n",
    "result['ssim_std'] = []\n",
    "result['psnr_mean'] = []\n",
    "result['psnr_std'] = []\n",
    "\n",
    "# set up test data dirs \n",
    "test_path = params['test_dir']\n",
    "HE_img_dir = \"{}{}\".format(test_path,'/HE_imgs/HE')\n",
    "IHC_img_dir = \"{}{}\".format(test_path,'/IHC_imgs/IHC')\n",
    "\n",
    "for epoch in range(params['num_epochs']):\n",
    "    \n",
    "    result['epoch'].append(epoch)\n",
    "    test_data = loader.stain_transfer_dataset(  epoch = epoch,\n",
    "                                                num_epochs = params['num_epochs'],\n",
    "                                                HE_img_dir = HE_img_dir,\n",
    "                                                IHC_img_dir = IHC_img_dir,\n",
    "                                                img_size= params['img_size'],\n",
    "                                                )\n",
    "    \n",
    "    test_data_loader = DataLoader(test_data, batch_size=1, shuffle=False) \n",
    "\n",
    "    ssim = StructuralSimilarityIndexMeasure(data_range=1.0)\n",
    "    ssim = ssim.cuda()\n",
    "    ssim_scores = []\n",
    "\n",
    "    psnr = PeakSignalNoiseRatio()\n",
    "    psnr = psnr.cuda()\n",
    "    psnr_scores = []\n",
    "\n",
    "    for i, (real_HE, real_IHC) in enumerate(test_data_loader):\n",
    "        \n",
    "        fake_IHC = model(real_HE)\n",
    "        print(i)\n",
    "        ssim_scores.append(ssim(fake_IHC, real_IHC))\n",
    "        psnr_scores.append(psnr(fake_IHC, real_IHC))\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    result['ssim_mean'].append(np.mean(ssim_scores))\n",
    "    result['ssim_std'].append(np.std(ssim_scores))\n",
    "\n",
    "    result['psnr_mean'].append(np.mean(psnr_scores))\n",
    "    result['psnr_std'].append(np.std(psnr_scores))\n",
    "\n",
    "# open file for writing\n",
    "f = open(output_folder_path,\"w\")\n",
    "\n",
    "# write file\n",
    "f.write( str(result) )\n",
    "\n",
    "# close file\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d960cb169598afdf3e07f36ef921bb5a4f876badc557409f77a22927d6dd7515"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
