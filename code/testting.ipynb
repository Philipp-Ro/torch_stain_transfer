{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import loader\n",
    "path_HE = 'C:/Users/phili/OneDrive/Uni/WS_22/Masterarbeit/Masterarbeit_Code_Philipp_Rosin/Data_set_BCI_challange/train/HE_imgs/HE'\n",
    "path_IHC = 'C:/Users/phili/OneDrive/Uni/WS_22/Masterarbeit/Masterarbeit_Code_Philipp_Rosin/Data_set_BCI_challange/train/IHC_imgs/IHC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_data = loader.stain_transfer_dataset( epoch = 1,\n",
    "                                                        num_epochs = 1,\n",
    "                                                        HE_img_dir = path_HE,\n",
    "                                                        IHC_img_dir = path_IHC,\n",
    "                                                        img_size=  [64,64],\n",
    "                                           )\n",
    "train_data_loader = DataLoader(train_data, batch_size=1, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HE_img, IHC_img = next(iter(train_data_loader))\n",
    "print (HE_img.shape)\n",
    "print (IHC_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "####### patch embedding ###########\n",
    "#   - in_channels are the input channels of the images for rgb = 3\n",
    "#   - patch_size is the sizes of the patches for the embedding \n",
    "#   - img_size is the size of the input image (has to be quadratic)\n",
    "#   - embedding_dim are the dimentions for the embedding with embedding_dim=0 it will be automaticly calculated \n",
    "    def __init__(self, in_channels, patch_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "# 1) the conv layer with kernel_size = stride writes out patches of the kernels the embedding_dim should be choosen as (patch_size**2)* in_channel so that no information is lost          \n",
    "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=self.embedding_dim,\n",
    "                                 kernel_size=patch_size,\n",
    "                                 stride=patch_size,\n",
    "                                 padding=0)\n",
    "\n",
    "# 2) flatten the feature map into 1D\n",
    "        self.flatten = nn.Flatten(start_dim=2,\n",
    "                                  end_dim=3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        \n",
    "        x_patched = self.patcher(x)\n",
    "        x_flattened = self.flatten(x_patched) \n",
    "# 3) permute the output tensor so that it has the form [batch_size, embedding_dim, num_patches]      \n",
    "        return x_flattened\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def getPositionEmbedding(embedding_dim, num_patches, n=10000):\n",
    "    # the n variable is scalling the values in the positional embedding in the attention is all you need paper n=10000 was choosen \n",
    "    p_embedding = torch.zeros((embedding_dim, num_patches))\n",
    "    p_embedding = p_embedding.cuda()\n",
    "    for k in range(embedding_dim):\n",
    "        for i in torch.arange(int(num_patches/2)):\n",
    "            denominator = np.power(n, 2*i/num_patches)\n",
    "            p_embedding[k, 2*i] = np.sin(k/denominator)\n",
    "            p_embedding[k, 2*i+1] = np.cos(k/denominator)\n",
    "    \n",
    "    return torch.unsqueeze(p_embedding, dim=0)\n",
    "\n",
    "x = getPositionEncoding(embedding_dim=764, num_patches=16, n=10)\n",
    "\n",
    "\n",
    "print(x.get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = PatchEmbedding(in_channels=3,patch_size=16, embedding_dim=764)\n",
    "patches.cuda()\n",
    "y = patches(HE_img)\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_model = embedding_dim\n",
    "# nhead = attention heads \n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer\n",
    "\n",
    "\n",
    "transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768,\n",
    "                                                       nhead=12,\n",
    "                                                       dim_feedforward=2048,\n",
    "                                                       dropout=0.1,\n",
    "                                                       activation=\"gelu\",\n",
    "                                                       batch_first=True,\n",
    "                                                       norm_first=True)\n",
    "\n",
    "transformer_decoder_layer = nn.TransformerDecoderLayer(d_model=768,\n",
    "                                                       nhead=12,\n",
    "                                                       dim_feedforward=2048,\n",
    "                                                       dropout=0.1,\n",
    "                                                       activation=\"gelu\",\n",
    "                                                       batch_first=True,\n",
    "                                                       norm_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_encoder = nn.TransformerEncoder(\n",
    "    encoder_layer=transformer_encoder_layer,\n",
    "    num_layers=12)\n",
    "\n",
    "transformer_decoder = nn.TransformerEncoder(\n",
    "    encoder_layer=transformer_decoder_layer,\n",
    "    num_layers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "###### GENERATOR CLASS #############\n",
    "# 1) set up the embedding of input img :\n",
    "#        - img_size is the size of the input inmage of the generator \n",
    "#        - embedding_dim : the dimentions used also in the transformer\n",
    "#        - patch_size : sizes of patches cut by the embedding [img_size % patch_size != 0]\n",
    "#        - create positional embedding\n",
    "#\n",
    "# 2) set up the the transformer encoder layer class :\n",
    "#        ---> https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer\n",
    "#        - the d_model = embedding_dim so that the dimentions of the embedded image and the transformer network match\n",
    "#        - nhead sets the number of heads for self attention in a transformer block \n",
    "#\n",
    "# 3) set up encoder class:\n",
    "#        --->  https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder\n",
    "#        - use the encoder-layer set up in 2) \n",
    "#        - num_layers defines the number of encoder-layers in the encoder\n",
    "\n",
    "    def __init__(self,img_size,embedding_dim, patch_size, in_channels, dropout_embedding, nhead,num_layers):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.dropout_embedding = dropout_embedding\n",
    "        self.nhead = nhead\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "\n",
    "        #### testting the compatebility for img_size and patch_site \n",
    "        if img_size % patch_size == 0:\n",
    "            self.patch_size = patch_size\n",
    "        else : \n",
    "            print('img_size / patch_size has to have no rest')\n",
    "\n",
    "        # number of patches in image for given patchsize \n",
    "        num_patches = (img_size * img_size) // patch_size**2 \n",
    "        # number of valiables in input image ( num_channels* img_height* img_width)\n",
    "        num_values = in_channels * img_size**2\n",
    "\n",
    "        if num_values % num_patches == 0:\n",
    "            self.embedding_dim =  int(num_values/num_patches)\n",
    "        else:\n",
    "            print('num_values / patch_num has to have no rest')\n",
    "\n",
    "        # create patches from the imput image the output by the PatchEmbedding is : [batch_size, num_patches, embedding_dim ]\n",
    "        # where as the embedding_dim is choosen as patch_size**2 * in_channels \n",
    "        self.patch_embedding = PatchEmbedding(in_channels=self.in_channels,patch_size=self.patch_size, embedding_dim=self.embedding_dim)\n",
    "    \n",
    "        self.num_patches = (img_size * img_size) // patch_size**2 \n",
    "        # positional embedding is added to keep the informatiuon of the order of the patches \n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, self.num_patches, self.embedding_dim))\n",
    "\n",
    "        self.embedding_dropout = nn.Dropout(p=self.dropout_embedding)\n",
    "\n",
    "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model= self.embedding_dim,\n",
    "                                                               nhead=self.nhead ,\n",
    "                                                               dim_feedforward=2048,\n",
    "                                                               dropout=0.1,\n",
    "                                                               activation=\"gelu\",\n",
    "                                                               batch_first=True,\n",
    "                                                               norm_first=True)\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "                                                    encoder_layer=self.transformer_encoder_layer,\n",
    "                                                    num_layers=self.num_layers)\n",
    "        \n",
    "        self.upsample = nn.PixelShuffle(self.num_patches)\n",
    "        \n",
    "        #self.linear = nn.Sequential(nn.Conv2d(self.embedding_dim, 3, 1, 1, 0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # Create the patch embedding\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        x = self.positional_embedding + x\n",
    "\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        x = x.view(1, self.embedding_dim, int(math.sqrt(self.num_patches)), int(math.sqrt(self.num_patches)))\n",
    "\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary \n",
    "\n",
    "vit = Generator(img_size= 64,\n",
    "                embedding_dim=0,\n",
    "                patch_size=16,\n",
    "                in_channels=3,\n",
    "                dropout_embedding=0.1,\n",
    "                nhead= 4,\n",
    "                num_layers=6\n",
    "                \n",
    "                )\n",
    "vit.cuda()\n",
    "fake_IHC = vit(HE_img)\n",
    "print(fake_IHC.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768,\n",
    "                                                       nhead=12,\n",
    "                                                       dim_feedforward=2048,\n",
    "                                                       dropout=0.1,\n",
    "                                                       activation=\"gelu\",\n",
    "                                                       batch_first=True,\n",
    "                                                       norm_first=True)\n",
    "transformer_encoder_layer.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "upsample = nn.PixelShuffle(16)\n",
    "x = fake_IHC.permute(0,2,1).view(1,768,4,4)\n",
    "y = upsample(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(1, 16, 768)\n",
    "\n",
    "x = x.permute(0, 2, 1)\n",
    "x = x.view(-1,3,64,64)\n",
    "print(x.shape)\n",
    "conv_layer = nn.Conv2d(in_channels =768,\n",
    "                       out_channels= 3,\n",
    "                       kernel_size= 1,\n",
    "                       stride = 1,\n",
    "                       padding= 0)\n",
    "\n",
    "out  = conv_layer (x)\n",
    "print (out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {}\n",
    "a['num'] = []\n",
    "for n in range (10):\n",
    "    a['num'].append(n)\n",
    "\n",
    "\n",
    "# open file for writing\n",
    "f = open(\"dict.txt\",\"w\")\n",
    "\n",
    "# write file\n",
    "f.write( str(a) )\n",
    "\n",
    "# close file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9294, device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(0., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(0.9294, device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(0., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(0.9294, device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(0., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(0.9294, device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(0., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(0.9294, device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(0., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(0.9294, device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(0., device='cuda:0', grad_fn=<MinBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch \n",
    "import loader \n",
    "import conv_models\n",
    "from torch.utils.data import DataLoader\n",
    "import utils\n",
    "from torchmetrics import StructuralSimilarityIndexMeasure\n",
    "from torchmetrics import PeakSignalNoiseRatio\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "gen_G_path = 'C:/Users/phili/OneDrive/Uni/WS_22/Masterarbeit/Masterarbeit_Code_Philipp_Rosin/torch_stain_transfer/experiment-results/Test_01/Generator_G_weights.pth'\n",
    "params = utils.get_config_from_yaml('C:/Users/phili/OneDrive/Uni/WS_22/Masterarbeit/Masterarbeit_Code_Philipp_Rosin/torch_stain_transfer/code/config.yaml')\n",
    "model = conv_models.GeneratorResNet(3, num_residual_blocks=6)\n",
    "\n",
    "model.load_state_dict(torch.load(gen_G_path))\n",
    "model =  model.cuda()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# set up result vector \n",
    "result = {}\n",
    "result['epoch'] = []\n",
    "result['ssim_mean'] = []\n",
    "result['ssim_std'] = []\n",
    "result['psnr_mean'] = []\n",
    "result['psnr_std'] = []\n",
    "\n",
    "# set up test data dirs \n",
    "test_path = params['test_dir']\n",
    "HE_img_dir = \"{}{}\".format(test_path,'/HE_imgs/HE')\n",
    "IHC_img_dir = \"{}{}\".format(test_path,'/IHC_imgs/IHC')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "test_data = loader.stain_transfer_dataset(  epoch = 1,\n",
    "                                                num_epochs = params['num_epochs'],\n",
    "                                                HE_img_dir = HE_img_dir,\n",
    "                                                IHC_img_dir = IHC_img_dir,\n",
    "                                                img_size= params['img_size'],\n",
    "                                                )\n",
    "    \n",
    "test_data_loader = DataLoader(test_data, batch_size=1, shuffle=False) \n",
    "\n",
    "ssim = StructuralSimilarityIndexMeasure(data_range=1.0)\n",
    "ssim = ssim.cuda()\n",
    "ssim_scores = []\n",
    "\n",
    "psnr = PeakSignalNoiseRatio()\n",
    "psnr = psnr.cuda()\n",
    "psnr_scores = []\n",
    "\n",
    "for i in range(6):\n",
    "    real_HE, real_IHC = next(iter(test_data_loader))\n",
    "        \n",
    "    fake_IHC = model(real_HE)\n",
    "    fake_IHC = fake_IHC +1\n",
    "\n",
    "    # step 2: convert it to [0 ,1]\n",
    "    fake_IHC = fake_IHC - fake_IHC.min()\n",
    "    fake_IHC_norm = fake_IHC / (fake_IHC.max() - fake_IHC.min())\n",
    "\n",
    "    ssim_scores.append(ssim(fake_IHC, real_IHC).item())\n",
    "    psnr_scores.append(psnr(fake_IHC, real_IHC).item())\n",
    "\n",
    "    print(torch.max(real_IHC))\n",
    "    print(torch.min(real_IHC))\n",
    "    print(torch.max(fake_IHC_norm))\n",
    "    print(torch.min(fake_IHC_norm))\n",
    "\n",
    "result['ssim_mean'].append(np.mean(ssim_scores))\n",
    "result['ssim_std'].append(np.std(ssim_scores))\n",
    "\n",
    "result['psnr_mean'].append(np.mean(psnr_scores))\n",
    "result['psnr_std'].append(np.std(psnr_scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_batch_plot(IHC_batch, HE_batch, fake_HE_batch, batch_size):\n",
    "    # prepare figures for plotting \n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    rows = batch_size\n",
    "    columns = 3\n",
    "    p=0\n",
    "\n",
    "    for n in range(batch_size):\n",
    "        p=p+1\n",
    "\n",
    "\n",
    "        fig.add_subplot(rows, columns, p)\n",
    "        #IHC_batch[n] = (IHC_batch[n] * 127.5 + 127.5).astype(np.uint8)\n",
    "        plt.imshow(IHC_batch[n] )\n",
    "        plt.axis('off')\n",
    "        plt.title('IHC_img_'+str(n))\n",
    "        p=p+1\n",
    "\n",
    "        fig.add_subplot(rows, columns, p)\n",
    "        #HE_batch[n] = (HE_batch[n] * 127.5 + 127.5).astype(np.uint8)\n",
    "        plt.imshow(HE_batch[n] )\n",
    "        plt.axis('off')\n",
    "        plt.title('HE_img_'+str(n))\n",
    "        p=p+1\n",
    "\n",
    "        fig.add_subplot(rows, columns, p)\n",
    "        #fake_HE_batch[n] = (fake_HE_batch[n] * 127.5 + 127.5).astype(np.uint8)\n",
    "        plt.imshow(fake_HE_batch[n] )\n",
    "        plt.axis('off')\n",
    "        plt.title('predicted_HE_img_'+str(n))\n",
    "\n",
    "        return fig\n",
    "\n",
    "\n",
    "def rescale_img_for_plot(IHC_batch,HE_batch,fake_HE_batch):\n",
    "    # rescale the picture range from [-1..1] to [0..255]\n",
    "    IHC_batch_256 = (IHC_batch * 127.5 + 127.5).astype(np.uint8)\n",
    "    HE_batch_256 = (HE_batch * 127.5 + 127.5).astype(np.uint8)\n",
    "    fake_HE_batch_256 = (fake_HE_batch * 127.5 + 127.5).astype(np.uint8)\n",
    "    \n",
    "    return IHC_batch_256,HE_batch_256,fake_HE_batch_256"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d960cb169598afdf3e07f36ef921bb5a4f876badc557409f77a22927d6dd7515"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
